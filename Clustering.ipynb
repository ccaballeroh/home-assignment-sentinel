{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook presents attempts to cluster text news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "data_path = Path(r'./data')\n",
    "files_data = {\n",
    "    file.stem : file for file in data_path.iterdir()\n",
    "}\n",
    "\n",
    "df_clustering = pd.read_json(files_data[\"clusters\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what is inside the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>lang</th>\n",
       "      <th>date</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.fondsk.ru/news/2020/03/24/litva-ne...</td>\n",
       "      <td>The coronavirus epidemic in Lithuania has clea...</td>\n",
       "      <td>Lithuania: No Physicians, No Food Stock - Stra...</td>\n",
       "      <td>eng</td>\n",
       "      <td>2020-03-24 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>MS fails to respond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.rubaltic.ru/article/politika-i-obs...</td>\n",
       "      <td>European experts say that the countries of Eas...</td>\n",
       "      <td>Coronavirus caused a catastrophe in the Baltic</td>\n",
       "      <td>eng</td>\n",
       "      <td>2020-03-19 08:50:37</td>\n",
       "      <td>0</td>\n",
       "      <td>MS fails to respond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://pandemya.ru/shveciya-otkazalas-ot-borby...</td>\n",
       "      <td>Sweden refused to fight against coronavirus: “...</td>\n",
       "      <td>Sweden abandoned the fight against coronavirus...</td>\n",
       "      <td>eng</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>MS fails to respond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://southfront.org/coronavirus-hysteria-hi...</td>\n",
       "      <td>Donate\\nDuring the past week, the center of th...</td>\n",
       "      <td>Coronavirus Hysteria Hits Russia As Europe Bec...</td>\n",
       "      <td>eng</td>\n",
       "      <td>2020-03-14 12:27:02</td>\n",
       "      <td>0</td>\n",
       "      <td>MS fails to respond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.fondsk.ru/news/2020/03/24/shvedski...</td>\n",
       "      <td>The Swedish oligarchs in conjunction with the ...</td>\n",
       "      <td>Swedish capital slows down in an epidemic and ...</td>\n",
       "      <td>eng</td>\n",
       "      <td>2020-03-24 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>MS fails to respond</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id  \\\n",
       "0  https://www.fondsk.ru/news/2020/03/24/litva-ne...   \n",
       "1  https://www.rubaltic.ru/article/politika-i-obs...   \n",
       "2  http://pandemya.ru/shveciya-otkazalas-ot-borby...   \n",
       "3  https://southfront.org/coronavirus-hysteria-hi...   \n",
       "4  https://www.fondsk.ru/news/2020/03/24/shvedski...   \n",
       "\n",
       "                                                text  \\\n",
       "0  The coronavirus epidemic in Lithuania has clea...   \n",
       "1  European experts say that the countries of Eas...   \n",
       "2  Sweden refused to fight against coronavirus: “...   \n",
       "3  Donate\\nDuring the past week, the center of th...   \n",
       "4  The Swedish oligarchs in conjunction with the ...   \n",
       "\n",
       "                                               title lang  \\\n",
       "0  Lithuania: No Physicians, No Food Stock - Stra...  eng   \n",
       "1     Coronavirus caused a catastrophe in the Baltic  eng   \n",
       "2  Sweden abandoned the fight against coronavirus...  eng   \n",
       "3  Coronavirus Hysteria Hits Russia As Europe Bec...  eng   \n",
       "4  Swedish capital slows down in an epidemic and ...  eng   \n",
       "\n",
       "                  date  cluster         cluster_name  \n",
       "0  2020-03-24 00:00:00        0  MS fails to respond  \n",
       "1  2020-03-19 08:50:37        0  MS fails to respond  \n",
       "2                 None        0  MS fails to respond  \n",
       "3  2020-03-14 12:27:02        0  MS fails to respond  \n",
       "4  2020-03-24 00:00:00        0  MS fails to respond  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clustering.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's display how many elements are there in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'MS fails to respond': 9,\n",
       "         'Anti-Russia': 8,\n",
       "         'Claims about China': 22,\n",
       "         'Collapse': 8,\n",
       "         'Coronavirus is not serious': 11,\n",
       "         'Cure': 6,\n",
       "         'EU fails to respond': 20,\n",
       "         'Miscellaneous': 20,\n",
       "         'Origins': 8,\n",
       "         'Properties': 6,\n",
       "         'Was predicted': 4,\n",
       "         'Secret plan of the global elite': 16,\n",
       "         'Ukraine fails to respond': 9,\n",
       "         'USA created COVID-2019': 34})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df_clustering[\"cluster_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if all the rows are populated with a title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>lang</th>\n",
       "      <th>date</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, text, title, lang, date, cluster, cluster_name]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clustering[df_clustering[\"title\"]==\"\".index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can shuffle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_clustering.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll get the titles in a list and the labels (clusters) in another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels = df[\"title\"].to_list(), df[\"cluster\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to cluster the documents by topic. For that, we'll do some preprocessing in the text (e.g., remove stop words and normalize the words--i.e., lemmatization in this case). We'll need a language model for doing that. To install spaCy's language model, first we need to download it in the console with: ```python -m download en_core_web_sm```. Once available, we can load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define next a custome tokenizer to pass to the ``CountVectorizer``. This tokenizer first process the text through a spaCy pipeline and returns a list of lemmas (normalized word) as long as the lemma is not part of the `STOP_WORDS` set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    \"\"\"Normalizes text and removes stop words.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    return([token.lemma_ for token in doc if token.text not in STOP_WORDS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer` takes texts as input and returns a term-document matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(tokenizer=tokenizer)\n",
    "X = count_vec.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181, 591)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit a KMeans estimator asking fro 14 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=14, random_state=24)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = KMeans(n_clusters=14, random_state=24)\n",
    "estimator.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the labels assigned by the clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8  2  2  0  8  2  0  0  3  3  1  3  5  0  3  3  6  0  3  2  0  3  3 13\n",
      "  3  3  0 12  5  1  1  0  7  2  2  6  2  0  6 10  3  5  2  0  2  2  2  2\n",
      "  3  2  8  8 13 12 13  2  3  2  2  8  6  5  3  2  2  3  5  8  2  3  2  2\n",
      "  2  0  3  3  3  3  3  2  2  3  3  2  3  2  0  0  1  2  3  2  6  2  8 13\n",
      "  3  3  0  9  5  3  0  0  6  3  0  0  2  4  0  3  2  2  2  5  1  6  1  0\n",
      " 13  0  2  3  3  1 13  5 10  0 10  3  3  2  0  3 12  6 11 12  2  3  3  3\n",
      "  0  8  2  0  0  8  3  3  1  6  8  8  2  2  2 12  3  3 12  2  2  3  3  0\n",
      "  3  2  2  3  2  3  3  0  5  0  8  0  3]\n"
     ]
    }
   ],
   "source": [
    "print(estimator.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare them agaisnt the real labels keeping in mind that the label numbers would not be the same. However, we can do a comparison element-wise to check if documents with the same cluster assigned match with documents that belonged to the same cluster in the original data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 6, 5, 4, 11, 13, 6, 13, 10, 2, 0, 6, 2, 4, 13, 0, 13, 13, 7, 11, 7, 5, 11, 13, 7, 12, 6, 0, 13, 6, 3, 2, 0, 2, 13, 13, 2, 3, 7, 1, 7, 13, 2, 3, 1, 13, 13, 2, 10, 13, 5, 11, 9, 11, 13, 13, 0, 3, 3, 11, 4, 11, 4, 3, 0, 11, 9, 8, 11, 1, 13, 2, 12, 7, 2, 2, 6, 7, 1, 7, 13, 13, 2, 7, 7, 8, 0, 2, 12, 9, 6, 13, 4, 4, 5, 13, 13, 1, 7, 8, 9, 6, 7, 6, 2, 13, 6, 8, 6, 6, 12, 12, 13, 2, 7, 6, 9, 2, 2, 1, 9, 11, 13, 7, 5, 2, 13, 7, 7, 2, 13, 12, 6, 6, 13, 8, 0, 7, 13, 6, 7, 8, 0, 2, 4, 11, 12, 6, 13, 11, 8, 10, 12, 2, 1, 11, 7, 6, 3, 11, 13, 12, 11, 6, 11, 4, 4, 6, 5, 13, 4, 13, 7, 1, 10, 3, 13, 2, 4, 8, 2]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is to use Latent Dirichlet Allocation (LDiA). This method would see each document as a combination of words (Bag-of-Words) where each word has a probability associated to each category or topic. We ask the algorithm to decompose each document into 14 \"topics\" when we `fit` the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=14, random_state=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=14, random_state=0)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can ask the model what is the probabilty distribution that a document belongs to each of the 14 topics. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00793653, 0.00793655, 0.37639588, 0.00793653, 0.00793654,\n",
       "        0.00793655, 0.00793651, 0.00793653, 0.00793653, 0.00793654,\n",
       "        0.00793653, 0.00793653, 0.52836569, 0.00793656]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.transform(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the highest probability is to topic with index 12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we can get the labels for each document and compare them against the original ones. Again, keeping in mind that the topics and the original labels don't necessarily match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12  5  4  4  2 13  8 11  7  2  2  1  5  4  6  7  3  1  3 13  2  1  1 13\n",
      "  4  1  5  0 13  2  4  8  2 10  7  0  5 11 13 12 12  6  3 13  5  7  0 13\n",
      "  3  2  4  2  1  7  7 11  3 11 13  2  3 10  3 13  7  1 13 11 13 10 13 10\n",
      " 11  8  3  1 11  1 12  6 11  1  7  3  3  8  3  8  4 13  4  9 13  9  4  7\n",
      "  8  1  1  3 10 12  7 10  3  1 13  7 10  5  3  8  0 10  5  5  2  0  2  2\n",
      "  6  0  7  0  2  3  0  0 12  9 12  6  9 10  8 11  8  5  2 10 13 13 13  3\n",
      "  8  2 12  3 12  2 12 12  3  9  2  2  1  5 11  1  1  5 12  7  4  3 11 13\n",
      " 11 13  1  3  1 13  7 11 13  1  2 10 13]\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(lda.transform(X), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 6, 5, 4, 11, 13, 6, 13, 10, 2, 0, 6, 2, 4, 13, 0, 13, 13, 7, 11, 7, 5, 11, 13, 7, 12, 6, 0, 13, 6, 3, 2, 0, 2, 13, 13, 2, 3, 7, 1, 7, 13, 2, 3, 1, 13, 13, 2, 10, 13, 5, 11, 9, 11, 13, 13, 0, 3, 3, 11, 4, 11, 4, 3, 0, 11, 9, 8, 11, 1, 13, 2, 12, 7, 2, 2, 6, 7, 1, 7, 13, 13, 2, 7, 7, 8, 0, 2, 12, 9, 6, 13, 4, 4, 5, 13, 13, 1, 7, 8, 9, 6, 7, 6, 2, 13, 6, 8, 6, 6, 12, 12, 13, 2, 7, 6, 9, 2, 2, 1, 9, 11, 13, 7, 5, 2, 13, 7, 7, 2, 13, 12, 6, 6, 13, 8, 0, 7, 13, 6, 7, 8, 0, 2, 4, 11, 12, 6, 13, 11, 8, 10, 12, 2, 1, 11, 7, 6, 3, 11, 13, 12, 11, 6, 11, 4, 4, 6, 5, 13, 4, 13, 7, 1, 10, 3, 13, 2, 4, 8, 2]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
